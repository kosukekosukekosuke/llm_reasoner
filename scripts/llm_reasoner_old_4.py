#!/usr/bin/env python3
"""
LLM-based Reasoner ROS Node
- „Åì„ÅÆ„Éé„Éº„Éâ„ÅØ„ÄåÊÑèÂë≥Êé®Ë´ñ„É¢„Ç∏„É•„Éº„É´„Äç„Å®„Åó„Å¶Áî®„ÅÑ„Çã
- Ê©üËÉΩ
    - YAML„Åã„Çâ„Éé„Éº„ÉâÊÉÖÂ†±„ÇíË™≠„ÅøËæº„ÇÄ (id, label, semantic, description)
    - ÊúÄÂ∞è„Éó„É≠„É≥„Éó„Éà„ÅßLLM„Å´„Éé„Éº„ÉâÁï™Âè∑„ÇíËøî„Åï„Åõ„Çã
    - token logprob„Åã„Çâ‰ø°È†ºÂ∫¶„ÇíË®àÁÆó
    - ÂÆâÂÖ®Ë®≠Ë®à: Êï∞Â≠óÊ§úË®º„Éª‰Ωé‰ø°È†ºÂ∫¶„É™„Éà„É©„Ç§„Éªfallback„Éé„Éº„Éâ
"""

import os
import re
import rospy
import yaml
import numpy as np
from std_msgs.msg import String
from llama_cpp import Llama


class LLMReasonerNode:
    def __init__(self):
        # --- ROS„Éé„Éº„ÉâÂàùÊúüÂåñ ---
        rospy.init_node("llm_reasoner", anonymous=False)

        # --- ROS„Éë„É©„É°„Éº„Çø ---
        self.model_path = rospy.get_param(
            "~model_path",
            os.path.join(os.path.dirname(__file__), "llama.cpp/models/meta-llama-3-8b-instruct.Q4_K_M.gguf")
        )
        self.map_yaml_path = rospy.get_param(
            "~map_yaml_path",
            "/home/amsl/catkin_ws/src/rover_navigator/map/graph/ikuta_graph.yaml"
        )
        self.n_ctx = rospy.get_param("~ctx_size", 2048)
        self.n_threads = rospy.get_param("~threads", 4)
        self.temperature = rospy.get_param("~temperature", 0.1)
        self.top_k = rospy.get_param("~top_k", 10)
        self.top_p = rospy.get_param("~top_p", 0.0)
        self.max_tokens = rospy.get_param("~max_tokens", 64)

        # --- Safety / retry parameters ---
        self.confidence_threshold = rospy.get_param("~confidence_threshold", 0.5)
        self.max_retries = rospy.get_param("~max_retries", 2)
        self.fallback_node = rospy.get_param("~fallback_node", None)  # Êú™ÊåáÂÆöÊôÇ„ÅØÊúÄÂàù„ÅÆ„Éé„Éº„Éâ

        # --- LLM„É¢„Éá„É´Ë™≠„ÅøËæº„Åø ---
        rospy.loginfo(f"üß© Loading LLM model from: {self.model_path}")
        self.llm = Llama(
            model_path=self.model_path,
            n_ctx=self.n_ctx,
            n_threads=self.n_threads,
            verbose=False,
            logits_all=True
        )

        # --- „Éé„Éº„Éâ„Éû„ÉÉ„ÉóË™≠„ÅøËæº„Åø ---
        self.node_ids, self.node_labels, self.node_semantics, self.node_descriptions = self.load_map(self.map_yaml_path)
        rospy.loginfo(f"üìÑ Loaded {len(self.node_ids)} nodes from map.")

        # fallback„Éé„Éº„ÉâÊú™ÊåáÂÆö„Å™„ÇâÊúÄÂàù„ÅÆ„Éé„Éº„Éâ„ÇíË®≠ÂÆö
        if self.fallback_node is None and self.node_ids:
            self.fallback_node = self.node_ids[0]

        # --- ROS„Éà„Éî„ÉÉ„ÇØË®≠ÂÆö ---
        self.sub = rospy.Subscriber("llm_prompt", String, self.callback)
        self.pub = rospy.Publisher("llm_result", String, queue_size=10)

        rospy.loginfo("‚úÖ LLM Reasoner Node initialized.")
        rospy.on_shutdown(self.cleanup)
        rospy.spin()

    # --------------------------
    def load_map(self, filename):
        """YAML„Åã„Çâ„Éé„Éº„ÉâÊÉÖÂ†±„ÇíÂèñÂæó: id, label, semantic, description"""
        if not os.path.exists(filename):
            rospy.logwarn(f"‚ö†Ô∏è Map file not found: {filename}")
            return [], [], [], []

        with open(filename, "r") as f:
            data = yaml.safe_load(f)

        ids, labels, semantics, descriptions = [], [], [], []
        for node in data.get("NODE", []):
            ids.append(node.get("id"))
            labels.append(node.get("label", "").strip())
            sem = node.get("semantic", [])
            semantics.append(", ".join(sem) if sem else "")
            descriptions.append(node.get("description", "").strip())
        return ids, labels, semantics, descriptions

    # --------------------------
    def make_prompt(self, user_query):
        """ÊúÄÂ∞èÈôê„ÅÆ„Éó„É≠„É≥„Éó„Éà„Çí‰ΩúÊàê: „Éé„Éº„Éâ‰∏ÄË¶ß + „Çø„Çπ„ÇØ + NUMBERÊåáÁ§∫"""
        options = "\n".join(
            f"{self.node_ids[i]}. {self.node_labels[i]} ({self.node_semantics[i]})"
            for i in range(len(self.node_ids))
        )
        prompt = (
            f"Available Locations:\n{options}\n"
            f"User Task: {user_query}\n"
            f"Answer with the NUMBER of the correct choice:"
        )
        return prompt

    # --------------------------
    def call_llm_once(self, prompt):
        """LLM„ÇíÂëº„Å≥Âá∫„Åó„Å¶ („ÉÜ„Ç≠„Çπ„ÉàÂá∫Âäõ, ‰ø°È†ºÂ∫¶) „ÇíËøî„Åô"""
        output = self.llm(
            prompt=prompt,
            temperature=self.temperature,
            top_k=self.top_k,
            top_p=self.top_p,
            max_tokens=self.max_tokens,
            stop=["###", "\n", "Instruction:"],
            echo=False,
            logprobs=1
        )
        text = output["choices"][0]["text"].strip()
        token_logprobs = output["choices"][0].get("logprobs", {}).get("token_logprobs", [])
        if token_logprobs:
            avg_logprob = np.mean(token_logprobs)
            confidence = float(np.exp(avg_logprob))  # Âπ≥Âùálogprob -> Á¢∫ÁéáËøë‰ºº
        else:
            confidence = None
        return text, confidence

    # --------------------------
    def validate_and_extract_number(self, text):
        """Êï∞Â≠ó„ÇíÊäΩÂá∫„Åó„ÄÅnode_ids„Å´Â≠òÂú®„Åô„Çã„ÅãÁ¢∫Ë™ç"""
        match = re.search(r'\b\d+\b', text)
        if not match:
            return None
        num = int(match.group(0))
        if num in self.node_ids:
            return num
        return None

    # --------------------------
    def callback(self, msg):
        """LLMÊé®Ë´ñ ‚Üí ‰ø°È†ºÂ∫¶Á¢∫Ë™ç ‚Üí „É™„Éà„É©„Ç§ ‚Üí fallback"""
        user_query = msg.data.strip()
        if not user_query:
            rospy.logwarn("‚ö†Ô∏è Á©∫„ÅÆ„Éó„É≠„É≥„Éó„Éà„ÇíÂèó‰ø°„Åó„Åæ„Åó„Åü")
            return

        rospy.loginfo(f"üß† User Task: {user_query}")

        prompt = self.make_prompt(user_query)
        rospy.loginfo(f"üìù Prompt to LLM:\n{prompt}")

        chosen_node = None
        chosen_conf = None

        for attempt in range(self.max_retries + 1):
            text, conf = self.call_llm_once(prompt)
            rospy.loginfo(f"üîÅ LLM output (try {attempt+1}): {text}")
            rospy.loginfo(f"   -> confidence: {conf:.3f}" if conf is not None else "   -> confidence: None")

            num = self.validate_and_extract_number(text)
            if num is not None and ((conf is None) or (conf >= self.confidence_threshold)):
                chosen_node = num
                chosen_conf = conf
                rospy.loginfo(f"‚úÖ Accept: node {num} (conf={conf})")
                break
            else:
                rospy.logwarn("‚ö†Ô∏è Invalid or low-confidence response, retrying...")

        # fallbackÂá¶ÁêÜ
        if chosen_node is None:
            last_num = None
            try:
                last_num = int(re.search(r'\b\d+\b', text).group(0))
            except Exception:
                last_num = None

            if last_num in self.node_ids:
                rospy.logwarn(f"‚ö†Ô∏è Low confidence but using last num: {last_num}")
                chosen_node = last_num
                chosen_conf = conf
            else:
                rospy.logerr(f"‚ùå Fail ‚Üí fallback to {self.fallback_node}")
                chosen_node = self.fallback_node
                chosen_conf = None

        self.pub.publish(str(chosen_node))
        rospy.loginfo(f"üì§ Publish: {chosen_node} (conf={chosen_conf})")

    # --------------------------
    def cleanup(self):
        rospy.loginfo("üßπ Shutting down LLM Reasoner Node...")
        try:
            del self.llm
            rospy.loginfo("‚úÖ Model resources released.")
        except Exception:
            rospy.logwarn("‚ö†Ô∏è Failed to clean up model resources.")


if __name__ == "__main__":
    try:
        LLMReasonerNode()
    except rospy.ROSInterruptException:
        pass
