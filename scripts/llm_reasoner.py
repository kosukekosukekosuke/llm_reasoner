#!/usr/bin/env python3

"""
LLM Reasoner „Éé„Éº„Éâ
- ÊÑèÂë≥ÊÉÖÂ†±‰ªò„Åç„Éé„Éº„ÉâÂú∞Âõ≥ÔºàYAMLÔºâ„Å®„É¶„Éº„Ç∂ÊåáÁ§∫ÔºàËá™ÁÑ∂Ë®ÄË™ûÔºâ„Åã„Çâ
  „Äå„Å©„ÅÆ„Éé„Éº„Éâ„Å´Âêë„Åã„ÅÜ„Åπ„Åç„Åã„Äç„Çí LLM „Å´ 1„Å§ÈÅ∏„Å∞„Åõ„Çã„ÄÇ
- „Åù„ÅÆÈÅ∏Êäû„Å´ÂØæ„Åó„Å¶‰ª•‰∏ã„ÇíË®àÁÆó„ÅóÔºå‰ø°È†ºÂ∫¶„ÇíË©ï‰æ°„Åô„ÇãÔºö
    - confidence: ÁîüÊàê„Éà„Éº„ÇØ„É≥Âàó„ÅÆÂπ≥Âùá logprob „ÅÆÊåáÊï∞
    - cosine: „ÇØ„Ç®„É™Êñá„Å®ÈÅ∏Êäû„Éé„Éº„ÉâÊÑèÂë≥ÊÉÖÂ†±„ÅÆÂüã„ÇÅËæº„Åø„Ç≥„Çµ„Ç§„É≥È°û‰ººÂ∫¶
- ‰ø°È†ºÂ∫¶„Åå‰Ωé„ÅÑÂ†¥Âêà„ÅØÂÜçÊé®Ë´ñ„É´„Éº„Éó„ÇíÂõû„ÅóÔºå
  semantic top-k + ÈÅéÂéªÂÄôË£ú„Å´Áµû„Å£„ÅüÂÄôË£úÈõÜÂêà„Åß LLM „Å´ÂÜçËÄÉ„Åï„Åõ„Çã„ÄÇ
- ÊúÄÁµÇÁöÑ„Å™Ê±∫ÂÆö„Éé„Éº„Éâ„Å®ÔºåÂêÑË©¶Ë°å„ÅÆ„É≠„Ç∞„Çí ROS Topic „Å´ publish „ÅóÔºå
  Âæå„Åã„ÇâÂÆüÈ®ìË©ï‰æ°ÔºàÊàêÂäüÁéá / ‰ø°È†ºÂ∫¶ / ÂÜçÊé®Ë´ñÂäπÊûú / AND vs weighted ÊØîËºÉÔºâ„Å´‰Ωø„Åà„ÇãÂΩ¢„ÅßÊÆã„Åô„ÄÇ
"""

import os
import re
import json
import math
import rospy
import yaml
import numpy as np

from std_msgs.msg import String, Int32, Float32
from llama_cpp import Llama

try:
    from sentence_transformers import SentenceTransformer
except ImportError:
    SentenceTransformer = None


class LLMReasonerNode:
    # =========================================================
    # ÂàùÊúüÂåñ
    # =========================================================
    def __init__(self):
        rospy.init_node("llm_reasoner", anonymous=False)

        # --- „Éë„ÇπÈñ¢ÈÄ£ ---
        self.map_yaml_path = rospy.get_param(
            "~map_yaml_path",
            "/home/amsl/catkin_ws/src/rover_navigator/map/graph/ikuta_graph_eval_v0.yaml",
        )
        self.model_path = rospy.get_param(
            "~model_path",
            os.path.join(
                os.path.dirname(__file__),
                "llama.cpp/models/Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf",
            ),
        )

        # --- LLMÁîüÊàê„Éë„É©„É°„Éº„Çø ---
        self.n_ctx = rospy.get_param("~ctx_size", 2048)
        self.n_threads = rospy.get_param("~threads", 4)
        self.temperature = rospy.get_param("~temperature", 0.1)
        self.top_k = rospy.get_param("~top_k", 10)
        self.top_p = rospy.get_param("~top_p", 0.0)
        self.max_tokens = rospy.get_param("~max_tokens", 64)

        # --- Âà§ÂÆö„É¢„Éº„Éâ / ‰ø°È†ºÂ∫¶„Éë„É©„É°„Éº„Çø ---
        # "and" or "weighted"
        self.decision_mode = rospy.get_param("~decision_mode", "weighted").lower()
        if self.decision_mode not in ("and", "weighted"):
            self.decision_mode = "weighted"

        # max_retries Âõû„Åæ„ÅßÂÜçÊé®Ë´ñÔºàÂêàË®àË©¶Ë°åÂõûÊï∞ = max_retries + 1Ôºâ
        self.max_retries = int(rospy.get_param("~max_retries", 2))

        self.conf_threshold = float(rospy.get_param("~conf_threshold", 0.5))
        self.cosine_threshold = float(rospy.get_param("~cosine_threshold", 0.5))
        self.combined_threshold = float(rospy.get_param("~combined_threshold", 0.6))
        self.alpha = float(rospy.get_param("~alpha", 0.5))  # weighted Áî®Èáç„Åø

        self.fallback_node = int(rospy.get_param("~fallback_node", 0))

        # --- Âüã„ÇÅËæº„ÅøÈñ¢ÈÄ£ ---
        self.use_embedding = bool(rospy.get_param("~use_embedding", True))
        self.embed_model_name = rospy.get_param("~embed_model_name", "BAAI/bge-m3")
        self.topk_semantic = int(rospy.get_param("~topk_semantic", 3))

        # --- Topic ÂêçÔºàÂü∫Êú¨Âõ∫ÂÆö„ÄÇÂøÖË¶Å„Å™„Çâ param „Åß‰∏äÊõ∏„ÅçÂèØÔºâ ---
        self.sub_query_topic = rospy.get_param("~sub_query", "/llm_reasoner/query")
        self.pub_node_topic = rospy.get_param(
            "~pub_node", "/llm_reasoner/chosen_node_id"
        )
        self.pub_conf_topic = rospy.get_param(
            "~pub_conf", "/llm_reasoner/confidence"
        )
        self.pub_cos_topic = rospy.get_param("~pub_cos", "/llm_reasoner/cosine")
        self.pub_meta_topic = rospy.get_param("~pub_meta", "/llm_reasoner/meta")

        # --- „Éé„Éº„ÉâÊÉÖÂ†±Ë™≠„ÅøËæº„Åø ---
        (
            self.node_ids,
            self.node_labels,
            self.node_semantics,
            self.node_descs,
        ) = self.load_map(self.map_yaml_path)

        if not self.node_ids:
            rospy.logerr("No nodes loaded from map. Abort.")
            raise SystemExit

        rospy.loginfo(f"üìÑ Loaded {len(self.node_ids)} nodes from map.")

        # ID -> index / label / semantic „ÅÆËæûÊõ∏
        self.id2idx = {int(nid): i for i, nid in enumerate(self.node_ids)}
        self.id2label = {
            int(nid): str(lbl) for nid, lbl in zip(self.node_ids, self.node_labels)
        }
        self.id2sem = {
            int(nid): str(sem)
            for nid, sem in zip(self.node_ids, self.node_semantics)
        }

        # --- LLM„É¢„Éá„É´Ë™≠„ÅøËæº„Åø ---
        rospy.loginfo(f"üß© Loading LLM model from: {self.model_path}")
        self.llm = Llama(
            model_path=self.model_path,
            n_ctx=self.n_ctx,
            n_threads=self.n_threads,
            logits_all=True,
            verbose=False,
        )

        # --- Âüã„ÇÅËæº„Åø„É¢„Éá„É´ÔºÜ„Éé„Éº„ÉâÂüã„ÇÅËæº„Åø ---
        self.embed_model = None
        self.node_embeds = None

        if self.use_embedding and SentenceTransformer is not None:
            try:
                device = "cpu"
                rospy.loginfo(
                    f"üß† Loading embedding model: {self.embed_model_name} (device={device})"
                )
                self.embed_model = SentenceTransformer(
                    self.embed_model_name, device=device
                )
                self.precompute_node_embeddings()
                rospy.loginfo("üß† Node embeddings ready.")
            except Exception as e:
                rospy.logwarn(
                    f"‚ö†Ô∏è Failed to initialize embedding model: {e}. Disable embeddings."
                )
                self.use_embedding = False
        else:
            if not SentenceTransformer:
                rospy.logwarn("‚ö†Ô∏è sentence_transformers is not available. Disable embeddings.")
            self.use_embedding = False

        # --- ROS Pub/Sub ---
        self.sub_query = rospy.Subscriber(
            self.sub_query_topic, String, self.callback_query
        )
        self.pub_node = rospy.Publisher(self.pub_node_topic, Int32, queue_size=10)
        self.pub_conf = rospy.Publisher(self.pub_conf_topic, Float32, queue_size=10)
        self.pub_cos = rospy.Publisher(self.pub_cos_topic, Float32, queue_size=10)
        self.pub_meta = rospy.Publisher(self.pub_meta_topic, String, queue_size=10)

        rospy.on_shutdown(self.on_shutdown)
        rospy.loginfo("‚úÖ LLM Reasoner Node ready.")
        rospy.spin()

    # =========================================================
    # „Éû„ÉÉ„Éó & Âüã„ÇÅËæº„Åø„É¶„Éº„ÉÜ„Ç£„É™„ÉÜ„Ç£
    # =========================================================
    def load_map(self, filename):
        if not os.path.exists(filename):
            rospy.logerr(f"Map YAML not found: {filename}")
            return [], [], [], []

        with open(filename, "r") as f:
            data = yaml.safe_load(f)

        ids, labels, semantics, descs = [], [], [], []
        for node in data.get("NODE", []):
            nid = int(node.get("id"))
            label = str(node.get("label", "")).strip()
            sem_list = node.get("semantic", [])
            sem = ", ".join(map(str, sem_list)) if sem_list else ""
            desc = str(node.get("description", "")).strip()
            ids.append(nid)
            labels.append(label)
            semantics.append(sem)
            descs.append(desc)
        return ids, labels, semantics, descs

    def precompute_node_embeddings(self):
        texts = []
        for nid in self.node_ids:
            i = self.id2idx[nid]
            label = self.node_labels[i]
            sem = self.node_semantics[i]
            desc = self.node_descs[i]
            txt = f"{label}. {sem}. {desc}".strip()
            texts.append(txt)
        self.node_embeds = self.embed_model.encode(
            texts,
            normalize_embeddings=True,
            show_progress_bar=False,
        )

    def encode_query(self, query_text):
        if not (self.use_embedding and self.embed_model):
            return None
        return self.embed_model.encode(
            query_text,
            normalize_embeddings=True,
            show_progress_bar=False,
        )

    def cosine_for_node(self, query_vec, node_id):
        if (query_vec is None) or (self.node_embeds is None):
            return None
        idx = self.id2idx.get(int(node_id))
        if idx is None:
            return None
        return float(np.dot(self.node_embeds[idx], query_vec))

    def topk_semantic_ids(self, query_vec, k):
        if (query_vec is None) or (self.node_embeds is None):
            return []
        k = max(1, min(int(k), len(self.node_ids)))
        sims = np.dot(self.node_embeds, query_vec)
        idxs = np.argsort(-sims)[:k]
        return [int(self.node_ids[i]) for i in idxs]

    # =========================================================
    # „Éó„É≠„É≥„Éó„ÉàÁîüÊàê
    # =========================================================
    def build_full_prompt(self, query_text):
        lines = ["Available locations:"]
        for nid, lbl, sem, desc in zip(
            self.node_ids, self.node_labels, self.node_semantics, self.node_descs
        ):
            lines.append(f"{nid}. {lbl} ({sem}) - {desc}")
        lines.append(f"User task: {query_text}")
        lines.append("Answer with the NUMBER of the correct choice:")
        return "\n".join(lines)

    def build_refine_prompt(self, query_text, candidate_ids):
        lines = ["Candidate locations:"]
        for nid in candidate_ids:
            i = self.id2idx[nid]
            lbl = self.node_labels[i]
            sem = self.node_semantics[i]
            desc = self.node_descs[i]
            lines.append(f"{nid}. {lbl} ({sem}) - {desc}")
        lines.append(f"User task: {query_text}")
        lines.append(
            "The previous decision was uncertain. "
            "Based on the user task and the candidate locations above, "
            "choose the single best destination."
        )
        lines.append("Answer with the NUMBER of the correct choice:")
        return "\n".join(lines)

    # =========================================================
    # LLM Âëº„Å≥Âá∫„Åó & Âá∫Âäõ„Éë„Éº„Çπ
    # =========================================================
    def call_llm(self, prompt):
        """
        LLM „Çí1ÂõûÂëº„Å≥Âá∫„Åô„ÄÇ
        Êàª„ÇäÂÄ§:
            raw_text: Áîü„ÉÜ„Ç≠„Çπ„Éà
            confidence: Âπ≥Âùá logprob „ÅÆ expÔºà„Å™„Åë„Çå„Å∞ NoneÔºâ
        """
        out = self.llm(
            prompt=prompt,
            temperature=self.temperature,
            top_k=self.top_k,
            top_p=self.top_p,
            max_tokens=self.max_tokens,
            stop=["Answer with the NUMBER of the correct choice:"],
            echo=False,
            logprobs=1,
        )

        text = out["choices"][0]["text"].strip()
        lp_list = out["choices"][0].get("logprobs", {}).get("token_logprobs", [])
        if lp_list:
            vals = [lp for lp in lp_list if lp is not None]
            if vals:
                avg_lp = float(sum(vals) / len(vals))
                conf = float(math.exp(avg_lp))
            else:
                conf = None
        else:
            conf = None
        return text, conf

    @staticmethod
    def extract_number(text):
        m = re.search(r"\b(\d+)\b", text)
        return int(m.group(1)) if m else None

    # =========================================================
    # Âçò‰∏ÄË©¶Ë°å„ÅÆÂà§ÂÆö„É≠„Ç∏„ÉÉ„ÇØ
    # =========================================================
    def judge_attempt(self, num, candidate_ids, conf, cos, combined):
        """
        1Âõû„ÅÆË©¶Ë°åÁµêÊûú„Å´ÂØæ„Åó„Å¶„Äåaccept or retry„Äç„ÇíÂà§ÂÆö„ÄÇ
        Êàª„ÇäÂÄ§:
            decision: "accept" or "retry"
            reject_reason: ‰∏çÊé°Áî®ÁêÜÁî± or None
            pass_conf, pass_cos, pass_combined: ÈñæÂÄ§Âà§ÂÆöÁµêÊûú
        """
        pass_conf = None
        pass_cos = None
        pass_combined = None
        reject_reason = None

        # Êï∞Â≠ó„Åå„Å™„ÅÑ/‰∏çÊ≠£
        if num is None:
            return "retry", "no_number", pass_conf, pass_cos, pass_combined
        if num not in candidate_ids:
            return "retry", "invalid_id", pass_conf, pass_cos, pass_combined

        # ÈñæÂÄ§Âà§ÂÆö
        if conf is not None:
            pass_conf = conf >= self.conf_threshold
        if cos is not None:
            pass_cos = cos >= self.cosine_threshold
        if combined is not None:
            pass_combined = combined >= self.combined_threshold

        # --- AND „É¢„Éº„Éâ ---
        if self.decision_mode == "and":
            conds = []
            if pass_conf is not None:
                conds.append(pass_conf)
            if pass_cos is not None:
                conds.append(pass_cos)

            if conds and all(conds):
                return "accept", None, pass_conf, pass_cos, pass_combined

            # ‰∏çÊé°Áî®ÁêÜÁî±
            if pass_conf is False and pass_cos is False:
                reject_reason = "low_both"
            elif pass_conf is False:
                reject_reason = "low_conf"
            elif pass_cos is False:
                reject_reason = "low_cos"
            else:
                reject_reason = "unclear"
            return "retry", reject_reason, pass_conf, pass_cos, pass_combined

        # --- weighted „É¢„Éº„Éâ ---
        if combined is not None and pass_combined is not None:
            if pass_combined:
                return "accept", None, pass_conf, pass_cos, pass_combined
            else:
                return "retry", "low_combined", pass_conf, pass_cos, pass_combined

        # combined „ÅåÁÑ°„ÅÑÂ†¥Âêà„ÅØ conf / cos „ÅÆ„Å©„Å°„Çâ„Åã„ÅåËâØ„Åë„Çå„Å∞Êé°Áî®
        if pass_conf is True or pass_cos is True:
            return "accept", None, pass_conf, pass_cos, pass_combined

        if pass_conf is False and pass_cos is False:
            reject_reason = "low_both"
        elif pass_conf is False:
            reject_reason = "low_conf"
        elif pass_cos is False:
            reject_reason = "low_cos"
        else:
            reject_reason = "unclear"

        return "retry", reject_reason, pass_conf, pass_cos, pass_combined

    # =========================================================
    # accept_reasonÔºàÊúÄÁµÇÁµêÊûú„É©„Éô„É´Ôºâ
    # =========================================================
    def classify_accept_reason(self, attempts_log, decision_source):
        if decision_source == "fallback":
            return "fallback"
        if decision_source == "last_num":
            return "last_num_salvage"
        if not attempts_log:
            return "unknown"

        # ÊúÄÂæå„Å´ accept „Åï„Çå„Åü attempt „ÇíÊé¢„Åô
        accept_attempts = [a for a in attempts_log if a["decision"] == "accept"]
        if not accept_attempts:
            return "unknown"
        last = accept_attempts[-1]

        def is_strong(a):
            if self.decision_mode == "and":
                return (a.get("pass_conf") is True) and (a.get("pass_cos") is True)
            else:
                if a.get("pass_combined") is True:
                    return True
                return (a.get("pass_conf") is True) and (a.get("pass_cos") is True)

        if last["attempt_index"] == 1:
            return "first_pass_strong" if is_strong(last) else "first_pass_partial"
        else:
            return "refine_strong" if is_strong(last) else "refine_weak"

    # =========================================================
    # „É°„Ç§„É≥Êé®Ë´ñ„É´„Éº„ÉóÔºà„Ç≥„Éº„É´„Éê„ÉÉ„ÇØÔºâ
    # =========================================================
    def callback_query(self, msg):
        query = msg.data.strip()
        if not query:
            rospy.logwarn("‚ö†Ô∏è Empty query received. Ignored.")
            return

        rospy.loginfo(f"üü¢ Inference start: '{query}'")

        # „ÇØ„Ç®„É™Âüã„ÇÅËæº„ÅøÔºàÂøÖË¶Å„Å™„ÇâÔºâ
        query_vec = self.encode_query(query) if self.use_embedding else None

        attempts_log = []
        chosen_id = None
        chosen_conf = None
        chosen_cos = None
        chosen_combined = None
        last_valid_num = None
        decision_source = None

        # Ë©¶Ë°å„É´„Éº„ÉóÔºà1ÂõûÁõÆ: full, 2ÂõûÁõÆ‰ª•Èôç: refineÔºâ
        for attempt_idx in range(1, self.max_retries + 2):
            if attempt_idx == 1:
                prompt_type = "full"
                candidate_ids = list(self.node_ids)
                prompt = self.build_full_prompt(query)
            else:
                prompt_type = "refine"
                sem_ids = (
                    self.topk_semantic_ids(query_vec, self.topk_semantic)
                    if query_vec is not None
                    else []
                )
                prev_ids = sorted(
                    {
                        a["parsed_num"]
                        for a in attempts_log
                        if a.get("parsed_num") is not None
                    }
                )
                cand = list({nid for nid in (sem_ids + prev_ids) if nid in self.id2idx})
                if not cand:
                    cand = list(self.node_ids)
                candidate_ids = cand
                prompt = self.build_refine_prompt(query, candidate_ids)

            # LLM Âëº„Å≥Âá∫„Åó
            raw, conf = self.call_llm(prompt)
            num = self.extract_number(raw)

            # cosine / combined
            cos = (
                self.cosine_for_node(query_vec, num)
                if (num is not None and query_vec is not None)
                else None
            )

            combined = None
            if (
                self.decision_mode == "weighted"
                and conf is not None
                and cos is not None
            ):
                combined = float(self.alpha * conf + (1.0 - self.alpha) * cos)
            elif conf is not None:
                combined = conf

            # Âà§ÂÆö
            decision, reject_reason, pass_conf, pass_cos, pass_combined = (
                self.judge_attempt(num, candidate_ids, conf, cos, combined)
            )

            # salvage Áî®
            if num is not None and num in candidate_ids:
                last_valid_num = num

            # „É≠„Ç∞Áî®Ë°®Á§∫ÂÄ§
            num_disp = str(num) if num is not None else "-"
            conf_disp = f"{conf:.3f}" if conf is not None else "-"
            cos_disp = f"{cos:.3f}" if cos is not None else "-"
            comb_disp = f"{combined:.3f}" if combined is not None else "-"

            if decision == "accept":
                emoji = "‚úÖ"
                rospy.loginfo(
                    f"  {emoji} Attempt {attempt_idx} [{prompt_type}] "
                    f"-> raw='{raw[:60]}' num={num_disp} "
                    f"conf={conf_disp} cos={cos_disp} comb={comb_disp} decision=accept"
                )
            else:
                emoji = "üîÅ"
                rospy.loginfo(
                    f"  {emoji} Attempt {attempt_idx} [{prompt_type}] "
                    f"-> raw='{raw[:60]}' num={num_disp} "
                    f"conf={conf_disp} cos={cos_disp} comb={comb_disp} "
                    f"decision=retry reason={reject_reason}"
                )

            # attempt „É≠„Ç∞‰øùÂ≠òÔºà„É°„ÇøÊÉÖÂ†±Áî®Ôºâ
            attempts_log.append(
                {
                    "attempt_index": attempt_idx,
                    "prompt_type": prompt_type,
                    "candidate_ids": candidate_ids,
                    "raw_output": raw,
                    "parsed_num": num,
                    "confidence": conf,
                    "cosine": cos,
                    "combined_score": combined,
                    "pass_conf": pass_conf,
                    "pass_cos": pass_cos,
                    "pass_combined": pass_combined,
                    "decision": "accept" if decision == "accept" else "retry",
                    "reject_reason": reject_reason if decision != "accept" else None,
                }
            )

            if decision == "accept":
                chosen_id = num
                chosen_conf = conf
                chosen_cos = cos
                chosen_combined = combined
                decision_source = "model"
                break

        # =====================================================
        # ÊúÄÁµÇÊ±∫ÂÆöÔºàsalvage / fallbackÔºâ
        # =====================================================
        if chosen_id is None:
            if last_valid_num is not None:
                rospy.logwarn(
                    f"‚ö†Ô∏è Using last valid number as salvage: {last_valid_num}"
                )
                chosen_id = int(last_valid_num)
                decision_source = "last_num"
            else:
                rospy.logerr(
                    f"‚ùå No valid response. Use fallback_node={self.fallback_node}."
                )
                chosen_id = int(self.fallback_node)
                decision_source = "fallback"

        accept_reason = self.classify_accept_reason(attempts_log, decision_source)

        # publish values
        label = self.id2label.get(chosen_id, "unknown")
        sem = self.id2sem.get(chosen_id, "")

        self.pub_node.publish(Int32(chosen_id))
        self.pub_conf.publish(
            Float32(chosen_conf if chosen_conf is not None else -1.0)
        )
        self.pub_cos.publish(Float32(chosen_cos if chosen_cos is not None else -1.0))

        rospy.loginfo(
            f"‚úÖ Inference done: node={chosen_id} ({label}), "
            f"semantics={sem}, accept_reason={accept_reason}, attempts={len(attempts_log)}"
        )

        # „É°„ÇøÊÉÖÂ†±ÔºàË©ï‰æ°Áî®Ôºâ
        meta = {
            "query": query,
            "node_id": int(chosen_id),
            "label": label,
            "label_semantics": sem,
            "confidence": chosen_conf,
            "cosine": chosen_cos,
            "combined_score": chosen_combined,
            "decision_mode": self.decision_mode,
            "accept_reason": accept_reason,
            "attempts": len(attempts_log),
            "fallback_used": decision_source == "fallback",
            "fallback_node": int(self.fallback_node),
            "decision_source": decision_source,
            "thresholds": {
                "conf_threshold": self.conf_threshold,
                "cosine_threshold": self.cosine_threshold,
                "combined_threshold": self.combined_threshold,
                "alpha": self.alpha,
            },
            "valid_ids": list(self.node_ids),
            "attempt_logs": attempts_log,
        }

        if self.use_embedding and self.node_embeds is not None and query_vec is not None:
            top_ids = self.topk_semantic_ids(query_vec, self.topk_semantic)
            top_entries = []
            for nid in top_ids:
                c = self.cosine_for_node(query_vec, nid)
                top_entries.append(
                    {
                        "node_id": int(nid),
                        "label": self.id2label.get(nid, ""),
                        "cosine": c,
                    }
                )
            meta["embed_diag"] = {
                "enabled": True,
                "model": self.embed_model_name,
                "text_mode": "label+semantic+description",
                "topk_semantic": top_entries,
            }
        else:
            meta["embed_diag"] = {"enabled": False}

        self.pub_meta.publish(String(json.dumps(meta)))

    # =========================================================
    # „Ç∑„É£„ÉÉ„Éà„ÉÄ„Ç¶„É≥
    # =========================================================
    def on_shutdown(self):
        rospy.loginfo("üßπ Shutting down LLM Reasoner Node...")
        try:
            del self.llm
        except Exception:
            pass
        rospy.loginfo("‚úÖ Done.")


if __name__ == "__main__":
    try:
        LLMReasonerNode()
    except rospy.ROSInterruptException:
        pass
